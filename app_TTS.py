import streamlit as st
import boto3
import json
import os
import io
from datetime import datetime
import time

# Streamlit é é¢é…ç½®
st.set_page_config(page_title="Voice Chat with AI", layout="wide")
st.title("Voice Chat with AI Assistant")

# åˆå§‹åŒ– AWS clients
bedrock = boto3.client(
    service_name='bedrock-runtime',
    region_name='us-west-2'
)

transcribe = boto3.client(
    service_name='transcribe',
    region_name='us-west-2'
)

s3 = boto3.client(
    service_name='s3',
    region_name='us-west-2'
)

polly = boto3.client(
    service_name='polly',
    region_name='us-west-2'
)


def upload_to_s3(audio_file, bucket_name='hackher'):
    """ä¸Šå‚³æ–‡ä»¶åˆ° S3"""
    try:
        file_name = f"audio_{datetime.now().strftime('%Y%m%d%H%M%S')}.wav"
        s3.upload_fileobj(audio_file, bucket_name, file_name)
        return f"s3://{bucket_name}/{file_name}"
    except Exception as e:
        st.error(f"ä¸Šå‚³åˆ° S3 æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        return None

def process_audio(audio_file):
    # ä¸Šå‚³åˆ° S3
    s3_uri = upload_to_s3(audio_file)
    if not s3_uri:
        return None
    
    # é–‹å§‹è½‰éŒ„ä»»å‹™
    try:
        job_name = f"transcribe_job_{datetime.now().strftime('%Y%m%d%H%M%S')}"
        
        response = transcribe.start_transcription_job(
            TranscriptionJobName=job_name,
            Media={'MediaFileUri': s3_uri},
            MediaFormat='wav',
            LanguageCode='zh-TW'
        )
        
        # ç­‰å¾…è½‰éŒ„å®Œæˆ
        with st.spinner('è½‰éŒ„é€²è¡Œä¸­...'):
            while True:
                status = transcribe.get_transcription_job(TranscriptionJobName=job_name)
                if status['TranscriptionJob']['TranscriptionJobStatus'] in ['COMPLETED', 'FAILED']:
                    break
                time.sleep(2)
        
        if status['TranscriptionJob']['TranscriptionJobStatus'] == 'COMPLETED':
            transcript_uri = status['TranscriptionJob']['Transcript']['TranscriptFileUri']
            # å¾ JSON ä¸­ç²å–å¯¦éš›çš„è½‰éŒ„æ–‡æœ¬
            import requests
            response = requests.get(transcript_uri)
            transcript_text = response.json()['results']['transcripts'][0]['transcript']
            return transcript_text
        else:
            st.error("è½‰éŒ„å¤±æ•—")
            return None
    except Exception as e:
        st.error(f"è™•ç†éŸ³é »æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        return None

def get_ai_response(input_text):
    try:
        body = json.dumps({
            "anthropic_version": "bedrock-2023-05-31",
            "max_tokens": 4096,
            "messages": [
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "text",
                            "text": input_text
                        }
                    ]
                }
            ],
            "temperature": 0.7,
            "top_p": 0.9,
        })
        response = bedrock.invoke_model(
            modelId='anthropic.claude-3-haiku-20240307-v1:0',
            body=body
        )

        response_body = json.loads(response.get('body').read())
        return response_body['content'][0]['text'] if 'content' in response_body else response_body['messages'][0]['content'][0]['text']
    
    except Exception as e:
        st.error(f"ç²å– AI å›æ‡‰æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        if 'response_body' in locals():
            if 'content' in response_body:
                return response_body['content'][0]['text']
            elif 'messages' in response_body:
                return response_body['messages'][0]['content'][0]['text']
        return None
def text_to_speech(text):
    """å°‡æ–‡å­—è½‰æ›ç‚ºèªéŸ³"""
    try:
        response = polly.synthesize_speech(
            Text=text,
            OutputFormat='mp3',
            VoiceId='Zhiyu',  # ä½¿ç”¨ä¸­æ–‡å¥³è²
            LanguageCode='cmn-CN'  # è¨­ç½®ç‚ºä¸­æ–‡
        )
        
        if "AudioStream" in response:
            # å°‡éŸ³é »æµè½‰æ›ç‚º bytes
            audio_stream = io.BytesIO(response['AudioStream'].read())
            return audio_stream
    except Exception as e:
        st.error(f"è½‰æ›èªéŸ³æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        return None
    

#v2
js_code = """
<script>
let recognitionRef = { current: null };
let silenceTimer = null;
let isListening = false;
let restartTimer = null;
let finalTranscript = '';

// ç­‰å¾… Streamlit çµ„ä»¶æº–å‚™å®Œæˆ
function waitForStreamlit() {
    return new Promise((resolve) => {
        const checkStreamlit = () => {
            if (window.Streamlit) {
                resolve();
            } else {
                setTimeout(checkStreamlit, 100);
            }
        };
        checkStreamlit();
    });
}

function setListening(status) {
    isListening = status;
    document.getElementById('status').innerHTML = status ? 'ğŸ¤ æ­£åœ¨è†è½...' : 'ğŸ›‘ å·²æš«åœ';
    document.getElementById('toggleBtn').textContent = status ? 'åœæ­¢éŒ„éŸ³' : 'é–‹å§‹éŒ„éŸ³';
}

async function setTranscript(text) {
    document.getElementById('final').innerHTML = text;
    // ç¢ºä¿ Streamlit å·²æº–å‚™å¥½
    if (text && text !== 'ç©ºçš„') {
        try {
            await waitForStreamlit();
            window.Streamlit.setComponentValue(text);
        } catch (error) {
            console.error('Streamlit é€šä¿¡éŒ¯èª¤:', error);
        }
    }
}

// é–‹å§‹èªéŸ³è¾¨è­˜
async function startListening() {
    const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
    if (!SpeechRecognition) {
        alert("æ­¤ç€è¦½å™¨ä¸æ”¯æ´èªéŸ³è¾¨è­˜ ğŸ˜¢");
        return;
    }

    const recognition = new SpeechRecognition();
    recognition.lang = "zh-TW";
    recognition.interimResults = true;
    recognition.continuous = true;

    recognition.onstart = () => {
        setListening(true);
        console.log("ğŸ¤ èªéŸ³è¾¨è­˜å·²å•Ÿå‹•");
    };

    recognition.onend = () => {
      setListening(false);
      console.log("ğŸ›‘ èªéŸ³è¾¨è­˜å·²çµæŸ");

      // ç­‰ä¸€æ®µæ™‚é–“å¾Œè‡ªå‹•é‡å•Ÿï¼ˆä¾‹å¦‚ï¼š1 ç§’ï¼‰
      restartTimer = setTimeout(() => {
        console.log("ğŸ”„ è‡ªå‹•é‡å•ŸèªéŸ³è¾¨è­˜...");
        startListening();
      }, 1000);
    };

    recognition.onresult = (event) => {
        // æ¸…é™¤ä¹‹å‰çš„éœéŸ³è¨ˆæ™‚å™¨
        if (silenceTimer) {
            clearTimeout(silenceTimer);
        }

        let interimTranscript = '';
        
        // ç´¯ç©æ‰€æœ‰çš„è­˜åˆ¥çµæœ
        for (let i = event.resultIndex; i < event.results.length; i++) {
            const transcript = event.results[i][0].transcript;
            if (event.results[i].isFinal) {
                finalTranscript += transcript + ' ';
            } else {
                interimTranscript += transcript;
            }
        }

        // é¡¯ç¤ºçµæœ
        const displayText = finalTranscript + interimTranscript;
        if (displayText.trim() === "") {
            setTranscript("ç©ºçš„");
        } else {
            setTranscript(displayText.trim());
        }

        // è¨­ç½® 2 ç§’éœéŸ³æª¢æ¸¬
        silenceTimer = setTimeout(() => {
            console.log("â±ï¸ 2 ç§’æœªèªªè©±ï¼Œæš«åœèªéŸ³è¾¨è­˜");
            if (finalTranscript.trim()) {
                setTranscript(finalTranscript.trim());
            }
            stopListening();
        }, 2000);
    };

    recognition.onerror = (event) => {
        console.error("èªéŸ³è¾¨è­˜éŒ¯èª¤:", event.error);
        setListening(false);
    };

    recognitionRef.current = recognition;
    recognition.start();
}

// åœæ­¢èªéŸ³è¾¨è­˜
function stopListening() {
    if (recognitionRef.current) {
        recognitionRef.current.stop();
        clearTimeout(silenceTimer);
        setListening(false);
        finalTranscript = ''; // é‡ç½®æœ€çµ‚æ–‡å­—
    }
}

// åˆ‡æ›èªéŸ³è¾¨è­˜
function toggleRecording() {
    if (!isListening) {
        finalTranscript = ''; // é–‹å§‹æ–°çš„éŒ„éŸ³æ™‚é‡ç½®æ–‡å­—
        startListening();
    } else {
        stopListening();
    }
}

// ç¢ºä¿ Streamlit å·²åŠ è¼‰
waitForStreamlit().then(() => {
    console.log('Streamlit å·²æº–å‚™å°±ç·’');
}).catch(error => {
    console.error('Streamlit åŠ è¼‰å¤±æ•—:', error);
});
</script>

<div>
    <button id="toggleBtn" onclick="toggleRecording()">é–‹å§‹éŒ„éŸ³</button>
    <div id="status">ğŸ›‘ å·²æš«åœ</div>
    <div>
        <p>è­˜åˆ¥çµæœï¼š<span id="final"></span></p>
    </div>
</div>
"""

def main():
    
    # ä½¿ç”¨ components.html ä¾†æ³¨å…¥ JavaScript ä»£ç¢¼
    st.components.v1.html(js_code, height=200)
    
    # æ¥æ”¶å¾ JavaScript å‚³ä¾†çš„è­˜åˆ¥çµæœ
    result = st.empty()
    
    if st.session_state.get("speech_result"):
        result.write(f"è­˜åˆ¥çµæœ: {st.session_state.speech_result}")
    # ç²å– AI å›æ‡‰
        with st.spinner('ç²å– AI å›æ‡‰ä¸­...'):
            ai_response = get_ai_response(st.session_state.speech_result)
            if ai_response:
                # åœ¨å³å´åˆ—é¡¯ç¤º AI å›æ‡‰
                st.header("AI å›æ‡‰")
                st.text_area("", ai_response, height=400)
                                            
                # è‡ªå‹•å°‡ AI å›æ‡‰è½‰æ›ç‚ºèªéŸ³ä¸¦æ’­æ”¾
                audio_stream = text_to_speech(ai_response)
                if audio_stream:
                    # ä½¿ç”¨ base64 ç·¨ç¢¼çš„æ–¹å¼ä¾†è‡ªå‹•æ’­æ”¾
                    import base64
                    audio_bytes = audio_stream.getvalue()
                    b64 = base64.b64encode(audio_bytes).decode()
                    
                    # å‰µå»ºè‡ªå‹•æ’­æ”¾çš„ HTML audio å…ƒç´ 
                    md = f"""
                        <audio id="myAudio" autoplay="true">
                            <source src="data:audio/mp3;base64,{b64}" type="audio/mp3">
                        </audio>
                        <script>
                            var audio = document.getElementById("myAudio");
                            audio.play();
                        </script>
                        """
                    st.markdown(md, unsafe_allow_html=True)
                    
                    # åŒæ™‚ä¹Ÿé¡¯ç¤ºä¸€å€‹å¯æ§çš„æ’­æ”¾å™¨
                    st.audio(audio_bytes)




if __name__ == "__main__":
    main()
